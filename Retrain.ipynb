{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Done reviewing all images!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2acacb5fd1464da9b1d389f80e44b00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Mark as Overlapping', style=ButtonStyle()), Button(bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "image_dir = \"/Users/rfalcao/Documents/FYP/ManualSegmentationAnns/cusp_images\"\n",
    "mask_dir = \"/Users/rfalcao/Documents/FYP/ManualSegmentationAnns/masks\"\n",
    "out_img_dir = \"/Users/rfalcao/Documents/FYP/ManualSegmentationAnns/overlapping/images\"\n",
    "out_mask_dir = \"/Users/rfalcao/Documents/FYP/ManualSegmentationAnns/overlapping/masks\"\n",
    "\n",
    "os.makedirs(out_img_dir, exist_ok=True)\n",
    "os.makedirs(out_mask_dir, exist_ok=True)\n",
    "\n",
    "# Get sorted list of filenames\n",
    "image_files = sorted([f for f in os.listdir(image_dir) if f.endswith((\".png\", \".jpg\"))])\n",
    "index = 0  # start from beginning or last saved index\n",
    "saved = 0\n",
    "\n",
    "def show_image():\n",
    "    global index\n",
    "    if index >= len(image_files):\n",
    "        print(\"âœ… Done reviewing all images!\")\n",
    "        return\n",
    "    \n",
    "    image_name = image_files[index]\n",
    "    mask_name = image_name.replace(\".jpg\", \".png\")\n",
    "\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    mask_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "    img = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img)\n",
    "    axs[0].set_title(\"Image\")\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(mask, cmap='gray')\n",
    "    axs[1].set_title(\"Mask\")\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"{image_name}\")\n",
    "    plt.show()\n",
    "\n",
    "def mark_overlapping(_):\n",
    "    global index, saved\n",
    "    image_name = image_files[index]\n",
    "    mask_name = image_name.replace(\".jpg\", \".png\")\n",
    "\n",
    "    # Copy to overlapping folder\n",
    "    shutil.copy(os.path.join(image_dir, image_name), os.path.join(out_img_dir, f\"overlap_{saved:04}.png\"))\n",
    "    shutil.copy(os.path.join(mask_dir, mask_name), os.path.join(out_mask_dir, f\"overlap_{saved:04}.png\"))\n",
    "\n",
    "    saved += 1\n",
    "    index += 1\n",
    "    update_display()\n",
    "\n",
    "def skip(_):\n",
    "    global index\n",
    "    index += 1\n",
    "    update_display()\n",
    "\n",
    "def update_display():\n",
    "    clear_output(wait=True)\n",
    "    show_image()\n",
    "    display(buttons_box)\n",
    "\n",
    "# Buttons\n",
    "btn_overlap = widgets.Button(description=\"Mark as Overlapping\", button_style='danger')\n",
    "btn_skip = widgets.Button(description=\"Skip\", button_style='success')\n",
    "btn_overlap.on_click(mark_overlapping)\n",
    "btn_skip.on_click(skip)\n",
    "\n",
    "buttons_box = widgets.HBox([btn_overlap, btn_skip])\n",
    "\n",
    "# Start\n",
    "update_display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished extracting patches. Total patches: 137\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Settings\n",
    "image_dir = \"/Users/rfalcao/Documents/FYP/Cusp Images_081224/annotations 4/cusp_images\"\n",
    "mask_dir = \"/Users/rfalcao/Documents/FYP/Cusp Images_081224/annotations 4/masks\"\n",
    "output_image_dir = \"/Users/rfalcao/Documents/FYP/Cusp Images_081224/annotations 4/patches128/images\"\n",
    "output_mask_dir = \"/Users/rfalcao/Documents/FYP/Cusp Images_081224/annotations 4/patches128/masks\"\n",
    "# overlap_names = set(os.listdir(\"/Users/rfalcao/Documents/FYP/ManualSegmentationAnns/overlapping/images\"))\n",
    "# overlap_names = {f.replace(\"overlap_\", \"\").replace(\".png\", \".jpg\") for f in overlap_names}\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(output_image_dir, exist_ok=True)\n",
    "os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "def extract_cusp_patches(image_name, image_id_start=0, expand_ratio=1.8):\n",
    "    image_path = os.path.join(image_dir, image_name)\n",
    "    mask_path = os.path.join(mask_dir, image_name.replace(\".jpg\", \".png\"))\n",
    "\n",
    "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "    binary_mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    # Connected component analysis\n",
    "    num_labels, labels = cv2.connectedComponents(binary_mask)\n",
    "    patch_id = image_id_start\n",
    "\n",
    "    for i in range(1, num_labels):  # skip background\n",
    "        component_mask = (labels == i).astype(np.uint8)\n",
    "        y_idxs, x_idxs = np.nonzero(component_mask)\n",
    "\n",
    "        if len(x_idxs) == 0 or len(y_idxs) == 0:\n",
    "            continue\n",
    "\n",
    "        # Compute bounding box around component\n",
    "        x_min, x_max = np.min(x_idxs), np.max(x_idxs)\n",
    "        y_min, y_max = np.min(y_idxs), np.max(y_idxs)\n",
    "\n",
    "        # Expand bounding box\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        new_width = int(width * expand_ratio)\n",
    "        new_height = int(height * expand_ratio)\n",
    "\n",
    "        x_center = (x_min + x_max) // 2\n",
    "        y_center = (y_min + y_max) // 2\n",
    "\n",
    "        # Compute final cropping area\n",
    "        x_start, x_end = x_center - new_width // 2, x_center + new_width // 2\n",
    "        y_start, y_end = y_center - new_height // 2, y_center + new_height // 2\n",
    "\n",
    "        # Compute required padding if out of bounds\n",
    "        pad_x_before = max(0, -x_start)\n",
    "        pad_x_after = max(0, x_end - image.shape[1])\n",
    "        pad_y_before = max(0, -y_start)\n",
    "        pad_y_after = max(0, y_end - image.shape[0])\n",
    "\n",
    "        # Apply padding\n",
    "        image_padded = np.pad(image, \n",
    "                              ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after), (0, 0)), \n",
    "                              mode='constant', constant_values=0)\n",
    "        mask_padded = np.pad(component_mask, \n",
    "                             ((pad_y_before, pad_y_after), (pad_x_before, pad_x_after)), \n",
    "                             mode='constant', constant_values=0)\n",
    "\n",
    "        # Adjust crop coordinates after padding\n",
    "        new_x_start = x_start + pad_x_before\n",
    "        new_x_end = x_end + pad_x_before\n",
    "        new_y_start = y_start + pad_y_before\n",
    "        new_y_end = y_end + pad_y_before\n",
    "\n",
    "        # Extract patches\n",
    "        patch_img = image_padded[new_y_start:new_y_end, new_x_start:new_x_end]\n",
    "        patch_mask = mask_padded[new_y_start:new_y_end, new_x_start:new_x_end]\n",
    "\n",
    "        # Save patches\n",
    "        img_out_path = os.path.join(output_image_dir, f\"image_patch_{patch_id:04}.png\")\n",
    "        mask_out_path = os.path.join(output_mask_dir, f\"mask_patch_{patch_id:04}.png\")\n",
    "\n",
    "        Image.fromarray(patch_img).save(img_out_path)\n",
    "        Image.fromarray((patch_mask * 255).astype(np.uint8)).save(mask_out_path)\n",
    "\n",
    "        patch_id += 1\n",
    "\n",
    "    return patch_id\n",
    "\n",
    "# Run on your dataset\n",
    "image_files = [f for f in os.listdir(image_dir) if f.endswith((\".png\", \".jpg\", \".tif\"))]\n",
    "next_id = 0\n",
    "for fname in image_files:\n",
    "    # if fname in overlap_names:\n",
    "    #     continue\n",
    "    next_id = extract_cusp_patches(fname, image_id_start=next_id)\n",
    "\n",
    "print(f\"âœ… Finished extracting patches. Total patches: {next_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Custom loss functions\n",
    "from metrics import log_dice_loss, dice_coef, iou  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Path to previously trained model\n",
    "pretrained_model_path = \"/Users/rfalcao/FYP/Train/UnetBC50tanh.h5\"  # Change to actual path\n",
    "\n",
    "# Load the model with custom metrics\n",
    "model = load_model(pretrained_model_path, custom_objects={'dice_coef': dice_coef, 'iou': iou})\n",
    "\n",
    "# Freeze early encoder layers to retain cusp knowledge\n",
    "for layer in model.layers[:10]:  # Adjust number of frozen layers if needed\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile with a lower learning rate for gradual fine-tuning\n",
    "model.compile(optimizer=Adam(learning_rate=1e-6), loss='binary_crossentropy', metrics=['binary_accuracy', dice_coef, iou])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Patches: 144 images\n",
      "Test Patches: 17 images\n"
     ]
    }
   ],
   "source": [
    "# Directories for new patches\n",
    "patch_image_dir = \"/Users/rfalcao/Documents/FYP/Cusp Images_081224/annotations 4/patches128/images\"  # Change to actual path\n",
    "patch_mask_dir = \"/Users/rfalcao/Documents/FYP/Cusp Images_081224/annotations 4/patches128/masks\"\n",
    "\n",
    "IMG_SIZE = 224  # Input size for U-Net\n",
    "\n",
    "# Get all patch filenames\n",
    "patch_filenames = sorted(os.listdir(patch_image_dir))  \n",
    "mask_filenames = set(os.listdir(patch_mask_dir))  \n",
    "\n",
    "# Split patches into train (80%) and validation set (20%)\n",
    "train_filenames, test_filenames = train_test_split(patch_filenames, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"Training Patches: {len(train_filenames)} images\")\n",
    "print(f\"Test Patches: {len(test_filenames)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Augmented Training Samples: 576\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation settings\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def load_and_augment_patches(filenames, image_dir, mask_dir, num_augments_per_image=4):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        mask_filename= filename.replace(\"image\", \"mask\")\n",
    "\n",
    "        mask_path = os.path.join(mask_dir, mask_filename) if mask_filename in mask_filenames else None\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Skipping {filename} (missing image)\")\n",
    "            continue\n",
    "\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        # Load mask or create an empty one\n",
    "        if mask_path and os.path.exists(mask_path):\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = mask.astype(np.float32) / 255.0\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "        else:\n",
    "            print(f\" {filename} (missing mask)\")\n",
    "            mask = np.zeros((img.shape[0], img.shape[1], 1), dtype=np.float32)\n",
    "\n",
    "        # Apply augmentation multiple times\n",
    "        for _ in range(num_augments_per_image):\n",
    "            seed = np.random.randint(10000)\n",
    "            aug_img = datagen.random_transform(img, seed=seed)\n",
    "            aug_mask = datagen.random_transform(mask, seed=seed)\n",
    "\n",
    "            # Resize after augmentation\n",
    "            aug_img = cv2.resize(aug_img, (IMG_SIZE, IMG_SIZE))\n",
    "            aug_mask = cv2.resize(aug_mask, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "            images.append(aug_img)\n",
    "            masks.append(aug_mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Load and augment patches\n",
    "X_train, y_train = load_and_augment_patches(train_filenames, patch_image_dir, patch_mask_dir, num_augments_per_image=4)\n",
    "\n",
    "\n",
    "print(f\"Total Augmented Training Samples: {len(X_train)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (576, 224, 224, 1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dataset for blank masks...\n",
      "âš ï¸ Found 55 blank masks out of 576 total.\n",
      "âŒ Your dataset contains 55 fully blank masks! Consider removing or handling them.\n"
     ]
    }
   ],
   "source": [
    "################### Check for blank masks ###################\n",
    "\n",
    "print(\"Checking dataset for blank masks...\")\n",
    "num_blank_masks = np.sum(np.all(y_train == 0, axis=(1,2)))  # Count fully blank masks\n",
    "print(f\"âš ï¸ Found {num_blank_masks} blank masks out of {len(y_train)} total.\")\n",
    "\n",
    "if num_blank_masks > 0:\n",
    "    print(f\"âŒ Your dataset contains {num_blank_masks} fully blank masks! Consider removing or handling them.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m126/126\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m495s\u001b[0m 4s/step - binary_accuracy: 0.6698 - dice_coef: 0.3135 - iou: 0.1079 - loss: 0.7014 - val_binary_accuracy: 0.7838 - val_dice_coef: 0.5781 - val_iou: 0.4629 - val_loss: 0.3758\n",
      "Epoch 2/20\n",
      "\u001b[1m103/126\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m1:29\u001b[0m 4s/step - binary_accuracy: 0.7756 - dice_coef: 0.5993 - iou: 0.4802 - loss: 0.3987"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Fine-tuning\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduce epochs to avoid overfitting\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Smaller batch size due to limited patch dataset\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.125\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-m2/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-m2/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-m2/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m ):\n\u001b[1;32m    219\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-m2/lib/python3.10/site-packages/tensorflow/python/data/ops/optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-m2/lib/python3.10/site-packages/tensorflow/python/ops/gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalHasValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Early stopping for efficiency\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "\n",
    "# Fine-tuning\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,  # Reduce epochs to avoid overfitting\n",
    "    batch_size=4,  # Smaller batch size due to limited patch dataset\n",
    "    validation_split=0.125,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save('BC50Tuned.h5')\n",
    "print(\"Fine-tuned model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Set Size: 17 images\n"
     ]
    }
   ],
   "source": [
    "def load_test_images(filenames, image_dir, mask_dir):\n",
    "    images, masks = [], []\n",
    "\n",
    "    for filename in filenames:\n",
    "        img_path = os.path.join(image_dir, filename)\n",
    "        \n",
    "        # âœ… Convert .jpg filename to .png for mask lookup\n",
    "        mask_filename= filename.replace(\"image\", \"mask\")\n",
    "        mask_path = os.path.join(mask_dir, mask_filename) if mask_filename in mask_filenames else None\n",
    "\n",
    "        # Load image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            print(f\"Skipping {filename} (missing image)\")\n",
    "            continue\n",
    "\n",
    "        # Resize image\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "        # Load mask or create an empty one\n",
    "        if mask_path and os.path.exists(mask_path):\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            mask = cv2.resize(mask, (IMG_SIZE, IMG_SIZE))\n",
    "            mask = mask.astype(np.float32) / 255.0\n",
    "            mask = np.expand_dims(mask, axis=-1)\n",
    "        else:\n",
    "            mask = np.zeros((IMG_SIZE, IMG_SIZE, 1), dtype=np.float32)\n",
    "\n",
    "        images.append(img)\n",
    "        masks.append(mask)\n",
    "\n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Load test images (no augmentation)\n",
    "X_test, y_test = load_test_images(test_filenames, patch_image_dir, patch_mask_dir)\n",
    "\n",
    "print(f\"Final Test Set Size: {len(X_test)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 706ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def predict_mask(image, model, target_size=(224, 224)):\n",
    "    \"\"\"Resize image, predict mask, and resize mask back to original size.\"\"\"\n",
    "    original_size = image.shape[:2]\n",
    "    \n",
    "    # Normalize and add channel + batch dimensions\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    resized_image = np.expand_dims(resized_image, axis=-1)  # Add channel dimension\n",
    "    resized_image = np.expand_dims(resized_image, axis=0)   # Add batch dimension\n",
    "    resized_image = resized_image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Predict mask\n",
    "    predicted_mask = model.predict(resized_image)[0, :, :, 0]\n",
    "    \n",
    "    # Resize mask back to original image size\n",
    "    predicted_mask = cv2.resize(predicted_mask, (original_size[1], original_size[0]))\n",
    "    \n",
    "    return predicted_mask\n",
    "\n",
    "def save_plot(image, true_mask, predicted_mask, save_dir, img_index):\n",
    "    \"\"\"Save the plot with original image, ground truth mask, and predicted mask.\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "    # Normalize predicted mask to [0,1] for visualization\n",
    "    predicted_mask = (predicted_mask + 1) / 2\n",
    "\n",
    "    # Create subplot\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # ğŸ”¹ Original Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # ğŸ”¹ Ground Truth Mask\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(true_mask, cmap='gray')\n",
    "    plt.title(\"Actual Mask (Ground Truth)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # ğŸ”¹ Predicted Mask with Probability Legend\n",
    "    plt.subplot(1, 3, 3)\n",
    "    mask_plot = plt.imshow(predicted_mask, cmap='jet')\n",
    "    plt.colorbar(mask_plot, fraction=0.046, pad=0.04)  # âœ… Add colorbar legend\n",
    "    plt.title(\"Predicted Mask (Probability)\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_dir, f\"prediction_{img_index}.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "    plt.close()  # Close the plot to free memory\n",
    "\n",
    "# âœ… Loop over test images\n",
    "save_directory = \"/Users/rfalcao/Documents/FYP/Cusp Images_081224/annotations 4/patches128/plots2\"  # ğŸ”¹ Change this to your desired save folder\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    img = X_test[i].squeeze()  # Original image\n",
    "    true_mask = y_test[i].squeeze()  # Ground truth mask\n",
    "\n",
    "    # Predict mask\n",
    "    predicted_mask = predict_mask(img, model)\n",
    "\n",
    "    # Save the plot\n",
    "    save_plot(img, true_mask, predicted_mask, save_directory, i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-m2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
